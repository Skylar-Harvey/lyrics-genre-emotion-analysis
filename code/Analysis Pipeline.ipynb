{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7950e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 1 - immporting needed libraries ###\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 2 - Loading the CSV files extracted earlier ###\n",
    "\n",
    "df_artist = pd.read_csv(\"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Kaggle Datasets\\\\DS_1\\\\artists-data.csv\")\n",
    "df_lyrics = pd.read_csv(\"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Kaggle Datasets\\\\DS_1\\\\lyrics-data.csv\")\n",
    "df_spot60k = pd.read_csv(\"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Kaggle Datasets\\\\DS_2\\\\Spotify Million Song Dataset_exported.csv\")\n",
    "df_genius = pd.read_csv(\"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Kaggle Datasets\\\\DS_3\\\\song_lyrics.csv\")\n",
    "df_list = [df_artist,df_lyrics,df_spot60k,df_genius] #list for easy access to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 3 - Basic inspections of new dataframes ###\n",
    "\n",
    "for df in df_list:\n",
    "\tprint('Dataframe Information:')\n",
    "\tprint(df.info())\n",
    "\tprint(df.shape)\n",
    "\tprint(df.columns)\n",
    "\tprint('Dataframe Samples:')\n",
    "\tprint(df.head())\n",
    "\tprint()\n",
    "\tprint()\n",
    "\tprint(\"END OF DATASET INFORMATON AND SAMPLE\")\n",
    "\tprint()\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a60301",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 4 - Checking for missing values in dataframes ###\n",
    "\n",
    "slice = 0\n",
    "for df in df_list:\n",
    "\tdf_names_list = [\"df_artist\",\"df_lyrics\",\"df_spot60k\",\"df_genius\"]\n",
    "\tprint(f'Missing values in {df_names_list[slice]}:')\n",
    "\tprint(df.isnull().sum())\n",
    "\tslice +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c27514",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 5 - Standardizing and adjusting column names ###\n",
    "\n",
    "df_list[0] = df_artist.rename(columns={\"Link\":\"Artist_Link\"})\n",
    "df_list[1] = df_lyrics.rename(columns={\n",
    "\t\"ALink\":\"Artist_Link\",\n",
    "\t\"SName\":\"Song_Name\",\n",
    "\t\"SLink\":\"Song_Link\",\n",
    "\t\"Lyric\":\"Lyrics\",\n",
    "\t\"language\":\"Language\"\n",
    "\t})\n",
    "df_list[2] = df_spot60k.rename(columns={\"song\":\"Song_Name\",\"text\":'Lyrics'})\n",
    "df_list[3] = df_genius.rename(columns={\n",
    "\t\"title\":\"Song_Name\",\n",
    "\t\"lyrics\":\"Lyrics\",\n",
    "\t\"language\":\"Language\",\n",
    "\t\"tag\":\"Genre\"\n",
    "\t})\n",
    "print(\"New column names for df_artist:\")\n",
    "print(df_list[0].columns)\n",
    "print(\"New column names for df_lyrics:\")\n",
    "print(df_list[1].columns)\n",
    "print(\"New column names for df_spot60k:\")\n",
    "print(df_list[2].columns)\n",
    "print(\"New column names for df_genius:\")\n",
    "print(df_list[3].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 6 - Adding genre information to spot60k dataset ###\n",
    "\n",
    "df_list[2][\"genre\"]=\"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed038c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 7 - Dropping columns not needed for analysis being done ###\n",
    "\n",
    "df_list[0] = df_list[0].drop(columns=[\"Popularity\"])\n",
    "df_list[1] = df_list[1].drop(columns=[\"Song_Link\"])\n",
    "df_list[2] = df_list[2].drop(columns=[\"link\",\"artist\"])\n",
    "df_list[3] = df_list[3].drop(columns=[\"artist\",\"year\",\"views\",\"features\",\"id\",\"language_cld3\",\"language_ft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 8 - Dropping rows with null values. ###\n",
    "\n",
    "df_list[0] = df_list[0].dropna(subset=[\"Artist\",\"Genres\",\"Songs\",\"Artist_Link\"])\n",
    "df_list[1] = df_list[1].dropna(subset=[\"Artist_Link\",\"Song_Name\",\"Lyrics\",\"Language\"])\n",
    "df_list[3] = df_list[3].dropna(subset=[\"Song_Name\",\"Language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fecc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 9 - Cleaning non-music values from df_genius dataframe ###\n",
    "\n",
    "df_list[3] = df_list[3][df_list[3][\"Genre\"] != \"misc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 10 - Establish a new merged dataframe linking df_artist and df_lyrics ###\n",
    "\n",
    "df_merged = df_list[1].merge(df_list[0], on=\"Artist_Link\", how=\"left\")\n",
    "df_list = [df_merged, df_list[2], df_list[3]]\n",
    "print(df_list[0].info())\n",
    "print(df_list[0].head())\n",
    "print(df_list[0].isnull().sum())\n",
    "print(df_list[0].shape)\n",
    "print(df_list[0][\"Language\"].value_counts()) #checking unique values of \"Language\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ff58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 11 - Cleaning up the new merged dataframe ###\n",
    "\n",
    "df_list[0] = df_list[0].dropna(subset=[\"Artist\",\"Genres\",\"Songs\"]) #drop rows with missing values\n",
    "df_list[0] = df_list[0].drop(columns=[\"Songs\"]) #drop column \"Songs\" from merged dataframe\n",
    "df_list[0] = df_list[0][df_list[0][\"Language\"]==\"en\"] #ensure only english language songs are in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 12 - Remaining steps for cleaning and normalizing data ###\n",
    "\n",
    "df_list[2] = df_list[2][df_list[2][\"Language\"]==\"en\"] #drop non-english data from genius dataframe\n",
    "df_list[1][\"Language\"]=\"en\" #adds a language column to spot60k data (all \"unknown\")\n",
    "df_list[0] = df_list[0].drop(columns=[\"Artist_Link\",\"Artist\"]) #droping \"artist link\" data from merged dataset\n",
    "df_list[0] = df_list[0].rename(columns={\"Genres\":\"Genre\"}) # Normalizing column names\n",
    "df_list[1] = df_list[1].rename(columns={\"genre\":\"Genre\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60eff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 13 - Forming concattinated master dataframe from all the cleaned data and checking features of the newly formed dataframe ###\n",
    "\n",
    "df_master = pd.concat([df_list[0],df_list[1],df_list[2]], ignore_index=True)\n",
    "print(\"COLUMNS IN DF_MASTER:\")\n",
    "print(df_master.columns)\n",
    "print(\"NUMBER OF NULL VALUES IN DF_MASTER:\")\n",
    "print(df_master.isnull().sum())\n",
    "print(\"INFORMATION FOR DF_MASTER:\")\n",
    "print(df_master.info())\n",
    "print()\n",
    "print(\"Duplicates in 'Song_Name' column: \", df_master.duplicated(subset=\"Song_Name\").sum())\n",
    "print(\"Duplicates in 'Lyrics' column: \", df_master.duplicated(subset=\"Lyrics\").sum())\n",
    "print(\"Duplicates in 'Song_Name' and 'Lyrics' column: \", df_master.duplicated(subset=[\"Song_Name\",\"Lyrics\"]).sum())\n",
    "print(\"VALUE COUNTS PER COLUMN OF DF_MASTER:\")\n",
    "print(\"Value counts for 'Language' column:\", df_master[\"Language\"].value_counts())\n",
    "print(\"Value counts for 'Genre' column:\", df_master[\"Genre\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 14 - Cleaning new master dataframe ###\n",
    "\n",
    "# Function for preprocessing lyrics for analysis\n",
    "stop_words = set(stopwords.words(\"english\")).union({\"im\",\"ive\",\"youre\",\"youve\",\"yeah\",\"oh\",\"get\",\"gonna\",\"aint\",\"uh\",\"ha\",\"wanna\",\"la\",\"hey\",\"woah\",\"whoa\",\"ooh\",\"mmm\"})\n",
    "def preprocess_lyrics(text):\n",
    "\tif isinstance(text, str):\n",
    "\t\ttext = text.lower()\n",
    "\t\ttext = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\t\ttext = re.sub(r\"\\d+\", \"\", text)\n",
    "\t\twords = text.split()\n",
    "\t\twords = [word for word in words if word not in stop_words]\n",
    "\t\treturn \" \".join(words)\n",
    "\treturn text\n",
    "\n",
    "# Function for processing and cleaning entries in the 'genre' column of the master dataframe\n",
    "genre_list = ['rock','pop','indie','rap','folk','blues','country','metal','electronic']\n",
    "def clean_genre(genre_string):\n",
    "\tif isinstance(genre_string, str):\n",
    "\t\tgenres = [g.strip().lower() for g in genre_string.split(';')]\n",
    "\t\tfor g in genres:\n",
    "\t\t\tif g in genre_list:\n",
    "\t\t\t\treturn g\n",
    "\treturn 'other'\n",
    "\n",
    "df_master[\"Lyrics_Clean\"] = df_master[\"Lyrics\"].apply(preprocess_lyrics) #cleaning up lyric data and inserting clean lyric data into new column\n",
    "df_master[\"Genre_Clean\"] = df_master[\"Genre\"].apply(clean_genre) #normalizes genre lables, and reasigns song entries with multiple lables a single genre lable based on first match to a desired list (genre_list)\n",
    "df_master_cln = df_master[[\"Song_Name\",\"Lyrics_Clean\",\"Genre_Clean\"]].copy() #creates a copy of the master data frame containing only the cleaned data and song names.\n",
    "df_master_cln = df_master_cln.drop(columns=[\"Song_Name\"]) #drops song_name data column from clean copy of master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 15 Performing word frequency analysis by genre ###\n",
    "\n",
    "# Function for preformance of word frequency anlaysis of words in different genres\n",
    "def word_frequency_by_genre(df,genre,top_n=25):\n",
    "\tgenre_lyrics = \" \".join(df[df[\"Genre_Clean\"] == genre][\"Lyrics_Clean\"]) #Gather all the lyrics of the specified genre and set them into on large string object\n",
    "\twords = genre_lyrics.split() #convert string into list of words\n",
    "\tword_counts = Counter(words) #count word frequency in genre\n",
    "\treturn word_counts.most_common(top_n) #return 20 most common words in genre\n",
    "\n",
    "# Function that performs frequency analysis for each genre\n",
    "def freq_by_genre(df,genre_list,top_n=25):\n",
    "\tresult ={}\n",
    "\tfor genre in genre_list:\n",
    "\t\tresult[genre] = word_frequency_by_genre(df, genre, top_n)\n",
    "\treturn result\n",
    "\n",
    "genre_list_2 = ['rock','pop','indie','rap','folk','blues','country','metal','electronic','other'] #redefine genre_list to include \"other\"\n",
    "freq_dict = freq_by_genre(df_master_cln,genre_list_2) #run frequency analysis on each genre in dataframe, returns top 25 results, stores results in dictionary\n",
    "for genre in genre_list_2:\n",
    "\tgenre_words = freq_dict[genre]\n",
    "\twords, counts = zip(*genre_words)\n",
    "\tplt.figure(figsize=(10,10))\n",
    "\tplt.barh(words,counts)\n",
    "\tplt.gca().invert_yaxis()\n",
    "\tplt.xlabel(\"Occurences in Lyric Data\")\n",
    "\tplt.ylabel(\"Top 25 Words\")\n",
    "\tplt.title(f\"Top 25 Words for {genre.capitalize()}\")\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c39fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 16 - Loading in dataset for EMOlex and converting it's contents into a format useful for running analysis\n",
    "\n",
    "df_emolex = pd.read_csv(\"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\NRC-Emotion-Lexicon\\\\NRC-Emotion-Lexicon\\\\NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\",\n",
    "\tsep=\"\\t\", engine=\"python\", header=None, names=[\"word\",\"emotion\",\"value\"])\n",
    "\n",
    "word_associations = defaultdict(list)\n",
    "for _, row in df_emolex.iterrows():\n",
    "\tif row[\"value\"]==1:\n",
    "\t\tword_associations[row[\"word\"]].append(row[\"emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd13b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 17 - Finding data on the emotional associations of the lyric data\n",
    "############# STEP 17.1 - Preparing for the analysis\n",
    "\n",
    "df_master_cln[\"Row_ID\"] = df_master_cln.index #creates a column of unique identifiers for each entry in the dataframe\n",
    "df_scores = pd.DataFrame(df_master_cln[\"Row_ID\"].copy()) #generates a new dataframe with the same amout rows as the cleaned master dataframe. both dataframes can be linked by the \"Row_ID\" key\n",
    "emo_tag_list = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"negative\",\"positive\",\"sadness\",\"surprise\",\"trust\"] #list of unique emotional tags in EMOlex associations list\n",
    "for tag in emo_tag_list: #expands df_scores to now have a column for each word in emo_tag_list. All are initialized to 0\n",
    "\tdf_scores[tag] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# STEP 17.2 - Scoring the lyrics data\n",
    "\n",
    "def score_lyrics(row_n): #define new function \"score_lyrics\". Function takes in one variable, an row number\n",
    "\ttoken_list = df_master_cln.loc[row_n,\"Lyrics_Clean\"].split() #create a list of word tokens from the lyric string for row_n\n",
    "\temotion_counter = {\"anger\":0,\"anticipation\":0,\"disgust\":0,\"fear\":0,\"joy\":0,\"negative\":0,\"positive\":0,\"sadness\":0,\"surprise\":0,\"trust\":0}\n",
    "\tfor word in token_list: #iterate through created list of word tokens\n",
    "\t\tif word in word_associations: #if a token is in the word_associations dictionary defined in step 16:\n",
    "\t\t\tword_key_values = word_associations[word] #pull the value for that word. This value is a list of associated emotions for that word\n",
    "\t\t\tfor emotion in word_key_values: #iterate through this list of emotions associated with that word\n",
    "\t\t\t\temotion_counter[emotion] +=1\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "\tfor tag in emo_tag_list:\n",
    "\t\tdf_scores.loc[row_n, tag] = emotion_counter[tag]\n",
    "\n",
    "for r in range(len(df_master_cln)):\n",
    "\tscore_lyrics(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb892ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# STEP 17.3 - Creating project checkpoint. Saving all created dataframes\n",
    "\n",
    "save_path_pkl = \"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Saved Dataframes\\\\pickles\\\\\"\n",
    "save_path_csv = \"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Saved Dataframes\\\\CSVs\\\\\"\n",
    "\n",
    "#saving to .pkl\n",
    "df_master.to_pickle(save_path_pkl + \"df_master.pkl\")\n",
    "df_master_cln.to_pickle(save_path_pkl + \"df_master_cln.pkl\")\n",
    "df_scores.to_pickle(save_path_pkl + \"df_scores.pkl\")\n",
    "#saving to .csv\n",
    "df_master.to_csv(save_path_csv + \"df_master.csv\")\n",
    "df_master_cln.to_csv(save_path_csv + \"df_master_cln.csv\")\n",
    "df_scores.to_csv(save_path_csv + \"df_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57995e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# STEP 17.4 - Normalizing the values across the rows in df_scores\n",
    "\n",
    "df_scores_normalized = pd.DataFrame(df_scores.copy())\n",
    "print(\"New dataframe created: 'df_scores_normalized'\")\n",
    "print(\"Data normalization initiated:\")\n",
    "def normalize_scores(row_n):\n",
    "\tscores_list = []\n",
    "\tnormal_scores = {\"anger\":0,\"anticipation\":0,\"disgust\":0,\"fear\":0,\"joy\":0,\"negative\":0,\"positive\":0,\"sadness\":0,\"surprise\":0,\"trust\":0}\n",
    "\tfor tag in emo_tag_list:\n",
    "\t\tscores_list.append(df_scores_normalized.loc[row_n,tag])\n",
    "\tscore_range = (max(scores_list)-min(scores_list))\n",
    "\tif score_range == 0:\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\tfor tag in emo_tag_list:\n",
    "\t\t\tnormal_scores[tag] = ((df_scores_normalized.loc[row_n,tag] - min(scores_list)) / score_range)\n",
    "\tfor tag in emo_tag_list:\n",
    "\t\tdf_scores_normalized.loc[row_n, tag] = normal_scores[tag]\n",
    "\n",
    "for r in range(len(df_scores)):\n",
    "\tif r % 100000 == 0:\n",
    "\t\tprint(f\"Rows processed : {r}\")\n",
    "\tnormalize_scores(r)\n",
    "print(\"Normalization complete!\")\n",
    "print(\"Printing sample ...\")\n",
    "print(df_scores_normalized.head(20))\n",
    "\n",
    "df_scores_normalized.to_pickle(save_path_pkl + \"df_scores_normalized.pkl\")\n",
    "df_scores_normalized.to_csv(save_path_csv + \"df_scores_normalized.csv\")\n",
    "print(\"This data has been stored as a .pkl file at:\")\n",
    "print(\"\t\t\" + save_path_pkl)\n",
    "print(\"This data has been stored as a .csv file at:\")\n",
    "print(\"\t\t\" + save_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Reloading pickeld dataframes##########\n",
    "NormScore_reloaded = pd.read_pickle(\"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Saved Dataframes\\\\pickles\\\\df_scores_normalized.pkl\")\n",
    "Master_cln_reloaded = pd.read_pickle(\"F:\\\\Docs\\\\personal\\\\projects\\\\Sentiment Analysis Project\\\\Saved Dataframes\\\\pickles\\\\df_master_cln.pkl\")\n",
    "##########Reloading pickeld dataframes##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 18 - Merging normalized scores and genre data from cleaned master data frame ###\n",
    "\n",
    "df_final = NormScore_reloaded.merge(Master_cln_reloaded[[\"Row_ID\",\"Genre_Clean\"]], on=\"Row_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 19 - Calculating the mean scores for each emotion and grouping by genre ###\n",
    "\n",
    "df_heatmap = df_final.groupby(\"Genre_Clean\")[emo_tag_list].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 20 - Generating a heatmap that shows the relationship of different genres to different emotion categories visually ###\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmp(\n",
    "\tdf_heatmap,\n",
    "\tcmap = \"YlGnBu\",\n",
    "\tannot = True,\n",
    "\tfmt = \".2f\",\n",
    "\tcbar = \"True\")\n",
    "\n",
    "plt.title(\"Average Emotional Profile of Genre\")\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
